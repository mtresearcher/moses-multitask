RDLM: relational dependency language model
------------------------------------------

This is a language model for the string-to-tree decoder with a dependency grammar.
It should work with any corpus with projective dependency annotation in ConLL format,
converted into the Moses format with the script mosesdecoder/scripts/training/wrappers/conll2mosesxml.py
It depends on NPLM for neural network training and querying.

Prerequisites
-------------

Install NPLM and compile moses with it. See the instructions in the Moses documentation for details:

  http://www.statmt.org/moses/?n=FactoredTraining.BuildingLanguageModel

Training
--------

RDLM is designed for string-to-tree decoding with dependency annotation on the target side.
If you have such a system, you can train RDLM on the target side of the same parallel corpus
that is used for training the translation model.

To train the model on additional monolingual data, or test it on some held-out test/dev data,
parse and process it in the same way that the parallel corpus has been processed.
This includes tokenization, parsing, truecasing, compound splitting etc.

RDLM is split into two neural network models, which can be trained with `train_model_head.sh` and `train_model_label.sh`
set the paths to NPLM, Moses, and the training/test files in the respective files, then execute:

  ./train_model_head.sh rdlm_head.nnlm working_dir_head
  ./train_model_label.sh rdlm_label.nnlm working_dir_label


Decoding
--------

To use RDLM during decoding, add the following line to your moses.ini config:

  [feature]
  RDLM path_head_lm=/path/to/rdlm_head.nnlm path_label_lm=/path/to/rdlm_label.nnlm context_up=2 context_left=3 context_right=0

  [weight]
  RDLM 0.1 0.1

Reference
---------

Sennrich, Rico (2015). Modelling and Optimizing on Syntactic N-Grams for Statistical Machine Translation.
  Transactions of the Association for Computational Linguistics.
